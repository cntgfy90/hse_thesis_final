{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b7c490e-a1cb-4f14-8b82-60c92e11ccfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/stepan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/stepan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/stepan/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, hamming_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from bert_dataset import BertDataset\n",
    "from preprocessing.utils import is_sentence_in_boundaries\n",
    "from datasets_utils import get_luxury_data, get_tech_data, get_retail_data, get_big_basket_data\n",
    "from preprocess import preprocess, with_category_features\n",
    "from bert import BERT\n",
    "from bert_dataset import BertDataset\n",
    "from bert_train import bert_train\n",
    "from bert_test import bert_test\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c61cb686-9bfb-4462-b64b-defcecc60698",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8efdd97-0f03-4117-b064-3004b078f190",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stepan/HSEPythonCourse/thesis/hse_thesis_final/src/preprocess.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[category] = data['category'].apply(lambda x: 1 if category in x else 0)\n",
      "/home/stepan/HSEPythonCourse/thesis/hse_thesis_final/src/preprocess.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[category] = data['category'].apply(lambda x: 1 if category in x else 0)\n",
      "/home/stepan/HSEPythonCourse/thesis/hse_thesis_final/src/preprocess.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[category] = data['category'].apply(lambda x: 1 if category in x else 0)\n",
      "/home/stepan/HSEPythonCourse/thesis/hse_thesis_final/src/preprocess.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[category] = data['category'].apply(lambda x: 1 if category in x else 0)\n",
      "/home/stepan/HSEPythonCourse/thesis/hse_thesis_final/src/preprocess.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[category] = data['category'].apply(lambda x: 1 if category in x else 0)\n",
      "/home/stepan/HSEPythonCourse/thesis/hse_thesis_final/src/preprocess.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[category] = data['category'].apply(lambda x: 1 if category in x else 0)\n"
     ]
    }
   ],
   "source": [
    "big_basket_data = get_big_basket_data()\n",
    "# luxury_data = get_luxury_data()\n",
    "# tech_data = get_tech_data()\n",
    "# retail_data = get_retail_data()\n",
    "\n",
    "datasets = [big_basket_data]\n",
    "dataset_names = ['Big basket']\n",
    "# datasets = [big_basket_data, retail_data, luxury_data, tech_data]\n",
    "# dataset_names = ['Big basket', 'Retail', 'Luxury', 'Tech']\n",
    "\n",
    "# Get datasets with description column preprocessed\n",
    "big_basket_data['description'] = big_basket_data['description'].apply(preprocess)\n",
    "# tech_data['description'] = tech_data['description'].apply(preprocess)\n",
    "# luxury_data['description'] = luxury_data['description'].apply(preprocess)\n",
    "# retail_data['description'] = retail_data['description'].apply(preprocess)\n",
    "\n",
    "# Preprocess categories\n",
    "big_basket_data = with_category_features(big_basket_data)\n",
    "# tech_data = with_category_features(tech_data)\n",
    "# luxury_data = with_category_features(luxury_data)\n",
    "# retail_data = with_category_features(retail_data)\n",
    "\n",
    "big_basket_data = big_basket_data[big_basket_data['description'].apply(lambda x: is_sentence_in_boundaries(x, max_tokens=200))]\n",
    "# retail_data = retail_data[retail_data['description'].apply(lambda x: is_sentence_in_boundaries(x, max_tokens=250))]\n",
    "# luxury_data = luxury_data[luxury_data['description'].apply(lambda x: is_sentence_in_boundaries(x, max_tokens=100))]\n",
    "# tech_data = tech_data[tech_data['description'].apply(lambda x: is_sentence_in_boundaries(x, max_tokens=200))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cf5f1e5-f4cf-4a09-ab45-9066ce2e8ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [column for column in big_basket_data.columns if column != 'description']\n",
    "\n",
    "big_basket_X_train, big_basket_X_test, big_basket_y_train, big_basket_y_test = train_test_split(\n",
    "    big_basket_data['description'],\n",
    "    big_basket_data[categories],\n",
    "    test_size=0.2,\n",
    "    random_state=13\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89773c60-2491-4a96-855f-95c4dd834364",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_basket_X_train = big_basket_X_train.reset_index()\n",
    "big_basket_X_train = big_basket_X_train.drop(['index'], axis=1)\n",
    "\n",
    "big_basket_X_test = big_basket_X_test.reset_index()\n",
    "big_basket_X_test = big_basket_X_test.drop(['index'], axis=1)\n",
    "\n",
    "big_basket_y_train = big_basket_y_train.reset_index()\n",
    "big_basket_y_train = big_basket_y_train.drop(['index'], axis=1)\n",
    "\n",
    "big_basket_y_test = big_basket_y_test.reset_index()\n",
    "big_basket_y_test = big_basket_y_test.drop(['index'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d802b264-9f4a-4cab-988f-5b306b498d2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((21156, 1), (5289, 1))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_basket_X_train.shape, big_basket_X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6dc920f5-ced6-4590-a05d-35484744d531",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "MAX_LEN = 512\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 1\n",
    "LEARNING_RATE = 1e-5\n",
    "NUM_CLASSES = len(big_basket_data.columns) - 1\n",
    "\n",
    "big_basket_train_dataset = BertDataset(big_basket_X_train, MAX_LEN, tokenizer, big_basket_y_train[categories])\n",
    "big_basket_test_dataset = BertDataset(big_basket_X_test, MAX_LEN, tokenizer, big_basket_y_test[categories])\n",
    "\n",
    "big_basket_train_loader = DataLoader(big_basket_train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "big_basket_test_loader = DataLoader(big_basket_test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c86a111-56c3-44cf-a6f7-a38828723092",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_basket_bert_model = BERT(NUM_CLASSES)\n",
    "# big_basket_bert_model.load_state_dict(torch.load('big_basket_bert_weights.bin', map_location=torch.device(device)))\n",
    "big_basket_bert_model = big_basket_bert_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64667b57-4451-4e55-b2d0-c3803df51005",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(outputs, targets):\n",
    "    return torch.nn.BCEWithLogitsLoss()(outputs, targets)\n",
    "\n",
    "optimizer = torch.optim.AdamW(params=big_basket_bert_model.parameters(),\n",
    "                  lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b35ec5ee-cf30-4109-8a19-2c0df0a0cafa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 500\n",
      "Epoch: 0, Loss:  0.10057325661182404\n",
      "Batch: 1000\n",
      "Epoch: 0, Loss:  0.07661626487970352\n",
      "Batch: 1500\n",
      "Epoch: 0, Loss:  0.06772053241729736\n",
      "Batch: 2000\n",
      "Epoch: 0, Loss:  0.07876542210578918\n",
      "Batch: 2500\n",
      "Epoch: 0, Loss:  0.09890714287757874\n",
      "Batch: 3000\n",
      "Epoch: 0, Loss:  0.07094499468803406\n",
      "Batch: 3500\n",
      "Epoch: 0, Loss:  0.06014532968401909\n",
      "Batch: 4000\n",
      "Epoch: 0, Loss:  0.057778216898441315\n",
      "Batch: 4500\n",
      "Epoch: 0, Loss:  0.0611313059926033\n",
      "Batch: 5000\n",
      "Epoch: 0, Loss:  0.06829587370157242\n"
     ]
    }
   ],
   "source": [
    "big_basket_train_result = bert_train(\n",
    "    model=big_basket_bert_model,\n",
    "    train_loader=big_basket_train_loader,\n",
    "    optimizer=optimizer,\n",
    "    loss_fn=loss_fn,\n",
    "    epochs=EPOCHS,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad134991-2613-4293-8b9d-3baf70222923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(big_basket_bert_model.state_dict(), \"big_basket_bert_weights.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11defbf5-bb20-4141-869a-41e7a97a9557",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9799)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(big_basket_train_result['accuracy'][0]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e45e741b-35ec-4865-8e15-95f00ebe8d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 100\n",
      "Batch: 200\n",
      "Batch: 300\n",
      "Batch: 400\n",
      "Batch: 500\n",
      "Batch: 600\n",
      "Batch: 700\n",
      "Batch: 800\n",
      "Batch: 900\n",
      "Batch: 1000\n",
      "Batch: 1100\n",
      "Batch: 1200\n",
      "Batch: 1300\n"
     ]
    }
   ],
   "source": [
    "big_basket_test_result = bert_test(\n",
    "    model=big_basket_bert_model,\n",
    "    validation_loader=big_basket_test_loader,\n",
    "    loss_fn=loss_fn,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7d2c5863-0a44-48be-9de1-e52336495622",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0792)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(big_basket_test_result['accuracy_subset']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5ba54843-0df7-4612-ba37-e7d48384a9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(big_basket_test_loader)\n",
    "# next(it)\n",
    "# next(it)\n",
    "# next(it)\n",
    "test_item = next(it)\n",
    "input_ids = test_item['ids'].to(device)\n",
    "attention_mask = test_item['mask'].to(device)\n",
    "token_type_ids = test_item['token_type_ids'].to(device)\n",
    "targets = test_item['targets'].cpu().detach().numpy()\n",
    "output = big_basket_bert_model(input_ids, attention_mask, token_type_ids)\n",
    " # add sigmoid, for the training sigmoid is in BCEWithLogitsLoss\n",
    "output = torch.sigmoid(output).cpu().detach().numpy().round()\n",
    "# thresholding at 0.5\n",
    "# output = output.flatten().round().numpy()\n",
    "# np.sum(output==targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e3c04e13-ccf2-4e89-b976-cc5cfe1ad2aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "aa6e5067-ff75-4358-a331-313b0eba8717",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f722f37e-fc1a-4518-92eb-60fdae61aec9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(targets, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9937e126-854d-4357-a383-4b033feafe95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "412"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_predictions = np.sum(output==targets)\n",
    "\n",
    "correct_predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
