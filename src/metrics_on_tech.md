ML-KNN:
    param_grid={
        'k': range(1,3),
        's': [0.5, 0.7, 1.0]
    }

ML-KNN (TF-IDF):
    best = MLkNN(k=1, s=0.5)

    Accuracy (subset): 0.8918025561921551
    Accuracy (ML): 0.9990699777695541
    Precision (macro): 0.8593072155313445
    Precision (micro): 0.918602861978766
    Recall (macro): 0.8627480407472846
    Recall (micro): 0.9193101324299353
    Hamming loss: 0.0009300222304459072

ML-KNN (Word2Vec):
    best = MLkNN(k=1, s=0.5)

    Accuracy (subset): 0.8887174966945791
    Accuracy (ML): 0.9989586930582187
    Precision (macro): 0.8474461188225638
    Precision (micro): 0.9084691054411312
    Recall (macro): 0.8564324252657364
    Recall (micro): 0.9101478287650139
    Hamming loss: 0.001041306941781315


ML-KNN (BERT Embeddings):
    best = MLkNN(k=1, s=0.5)

    Accuracy (subset): 0.8913618334067871
    Accuracy (ML): 0.9990390653497386
    Precision (macro): 0.855330116138894
    Precision (micro): 0.9153349723417332
    Recall (macro): 0.8596435650225405
    Recall (micro): 0.9173082845703726
    Hamming loss: 0.0009609346502612982

Classifier Chain:
    param_grid=[
        {
            'classifier': [MultinomialNB()],
            'classifier__alpha': [0.7],
        },
        {
            'classifier': [DecisionTreeClassifier()],
            'classifier__criterion': ['log_loss'],
        },
    ]

    best = {
        'classifier': DecisionTreeClassifier(),
        'classifier__criterion': 'log_loss'
    }

Classifier Chain (TF-IDF):
    Accuracy (subset): 0.8913618334067871
    Accuracy (ML): 0.9991194376412587
    Precision (macro): 0.8751858191848155
    Precision (micro): 0.9248068006182381
    Recall (macro): 0.8696999252314739
    Recall (micro): 0.9213889744379428
    Hamming loss: 0.0008805623587412816

Classifier Chain (Word2Vec):
    Accuracy (subset): 0.8884971353018951
    Accuracy (ML): 0.9987069776397218
    Precision (macro): 0.8191524466191092
    Precision (micro): 0.8734224201930215
    Recall (macro): 0.8538772066767027
    Recall (micro): 0.9058361564521097
    Hamming loss: 0.0012930223602780704

Classifier Chain (BERT Embeddings):
    Accuracy (subset): 0.8889378580872631
    Accuracy (ML): 0.9987529046634477
    Precision (macro): 0.8144362520560097
    Precision (micro): 0.8802753666566896
    Recall (macro): 0.8544029577460736
    Recall (micro): 0.9057591623036649
    Hamming loss: 0.0012470953365523467

CatBoost:
    grid = {
        'iterations': [30, 60],
        'loss_function': ['MultiLogloss', 'MultiCrossEntropy'],
        'allow_const_label': [True],
        'random_state': [13],
    }

    best = {
        'allow_const_label': True,
        'iterations': 60,
        'loss_function': 'MultiLogloss',
        'random_state': 13
    }

CatBoost (TF-IDF):
    Accuracy (subset): 0.0370207139709123
    Accuracy (ML): 0.9950098522298012
    Precision (macro): 0.5800204278958203
    Precision (micro): 0.9093113482056256
    Recall (macro): 0.16148175760401037
    Recall (micro): 0.14436402833384662
    Hamming loss: 0.004990147770198837

CatBoost (Word2Vec):
    Accuracy (subset): 0.6077567210224769
    Accuracy (ML): 0.9986305798021782
    Precision (macro): 0.9233314530371447
    Precision (micro): 0.9887296094908552
    Recall (macro): 0.8295964478358093
    Recall (micro): 0.7700184785956268
    Hamming loss: 0.0013694201978218226

CatBoost (BERT Embeddings):
    Accuracy (subset): 0.5592772146319964
    Accuracy (ML): 0.9984362731633385
    Precision (macro): 0.9235904318110776
    Precision (micro): 0.9879144716454912
    Recall (macro): 0.8208206192710897
    Recall (micro): 0.7363720357252849
    Hamming loss: 0.0015637268366614234

CatBoost (Default):
    Accuracy (subset): 0.13089466725429705
    Accuracy (ML): 0.9962004219986911
    Precision (macro): 0.3272404456520776
    Precision (micro): 0.8659432387312187
    Recall (macro): 0.1393918603086439
    Recall (micro): 0.3993686479827533
    Hamming loss: 0.00379957800130892

RakelO:
    param_grid=[
        {
            'base_classifier': [MultinomialNB()],
            'base_classifier__alpha': [0.7],
        },
        {
            'base_classifier': [DecisionTreeClassifier()],
            'base_classifier__criterion': ['log_loss'],
        },
    ]

    best = {
        'base_classifier': DecisionTreeClassifier(criterion='log_loss'),
        'base_classifier_require_dense': [True, True],
        'labelset_size': big_basket_y_train.to_numpy().shape[1],
        'model_count': 6,
    }

RakelO (TF-IDF):
    Accuracy (subset): 0.8891582194799471
    Accuracy (ML): 0.9990646784975856
    Precision (macro): 0.8805824161317456
    Precision (micro): 0.9366864856178692
    Recall (macro): 0.8513876420426756
    Recall (micro): 0.8975977825685247
    Hamming loss: 0.00093532150241426

RakelO (Word2Vec):
    Accuracy (subset): 0.8884971353018951
    Accuracy (ML): 0.9993667369997818
    Precision (macro): 0.9228556768932341
    Precision (micro): 0.993170565135735
    Recall (macro): 0.8503068599451714
    Recall (micro): 0.8957499230058515
    Hamming loss: 0.0006332630002181534

RakelO (BERT Embeddings):
    Accuracy (subset): 0.8893785808726311
    Accuracy (ML): 0.9993795435737054
    Precision (macro): 0.92253043872608
    Precision (micro): 0.994619523443505
    Recall (macro): 0.8508455460478728
    Recall (micro): 0.8966738527871881
    Hamming loss: 0.0006204564262946343

BERT:
    Correct predictions (499 classes):
        [4509., 4535., 4476., 4516., 4533., 4523., 4515., 4537., 4494.,
            4522., 4532., 4533., 4530., 4534., 4537., 4533., 4524., 4538.,
            4499., 4488., 4538., 4516., 4504., 4521., 4516., 4533., 4522.,
            4538., 4528., 4536., 4538., 4537., 4534., 4538., 4499., 4536.,
            4511., 4522., 4529., 4490., 4536., 4529., 4537., 4519., 4538.,
            4508., 4530., 4524., 4490., 4535., 4457., 4534., 4536., 4528.,
            4528., 4530., 4534., 4532., 4500., 4502., 4532., 4529., 4524.,
            4536., 4534., 4537., 4531., 4499., 4534., 4462., 4535., 4538.,
            4527., 4537., 4533., 4528., 4530., 4534., 4525., 4474., 4534.,
            4534., 4531., 4533., 4507., 4318., 4491., 4537., 4470., 4535.,
            4531., 4529., 4518., 4529., 4529., 4479., 4537., 4538., 4503.,
            4535., 4530., 4537., 4530., 4415., 4524., 4529., 4534., 4538.,
            4534., 4382., 4533., 4525., 4538., 4423., 4525., 4315., 4536.,
            4482., 4536., 4510., 4538., 4516., 4350., 4521., 4538., 4507.,
            4532., 4538., 4522., 4523., 4534., 4537., 4537., 4500., 4529.,
            4532., 4523., 4487., 4513., 4538., 4533., 4534., 4538., 4328.,
            4525., 4537., 4537., 4525., 4532., 4262., 4519., 4533., 4524.,
            4477., 4523., 4537., 4527., 4527., 4520., 4495., 4530., 4525.,
            4537., 4536., 4456., 4509., 4533., 4497., 4528., 4537., 4533.,
            4522., 4487., 4459., 4518., 4499., 4502., 4516., 4537., 4537.,
            4506., 4509., 4536., 4531., 4527., 4536., 4532., 4380., 4370.,
            4533., 4520., 4520., 4537., 4415., 4500., 4534., 4494., 4534.,
            4445., 4536., 4517., 4516., 4458., 4535., 4534., 4534., 4527.,
            4537., 4501., 4493., 4533., 4534., 4520., 4524., 4537., 4496.,
            4482., 4518., 4524., 4518., 4533., 4483., 4501., 4531., 4525.,
            4536., 4529., 4492., 4538., 4459., 4534., 4474., 4538., 4508.,
            4537., 4441., 4524., 4509., 4525., 4286., 4509., 4491., 4532.,
            4528., 4538., 4526., 4531., 4537., 4514., 4490., 4535., 4505.,
            4538., 4502., 4529., 4528., 4411., 4495., 4519., 4532., 4516.,
            4523., 4537., 4528., 4521., 4537., 4476., 4538., 4515., 4537.,
            4537., 4531., 4529., 4535., 4535., 4529., 4531., 4538., 4439.,
            4532., 4518., 4507., 4536., 4535., 4537., 4509., 4525., 4536.,
            4511., 4537., 4533., 4538., 4536., 4520., 4249., 4538., 4530.,
            4465., 4531., 4538., 4538., 4532., 4520., 4482., 4510., 4528.,
            4407., 4530., 4507., 4538., 4532., 4535., 4535., 4509., 4528.,
            4515., 4492., 4356., 4535., 4536., 4500., 4512., 4538., 4524.,
            4265., 4537., 4536., 4537., 4528., 4515., 4537., 4531., 4512.,
            4536., 4534., 4522., 4535., 4525., 4536., 4534., 4531., 4538.,
            4518., 4508., 4511., 4532., 4515., 4518., 4532., 4536., 4535.,
            4483., 4309., 4534., 4525., 4528., 4532., 4495., 4528., 4523.,
            4515., 4511., 4537., 4528., 4526., 4534., 4529., 4284., 4527.,
            4525., 4503., 4529., 4536., 4520., 4503., 4536., 4536., 4535.,
            4518., 4532., 4534., 4537., 4528., 4527., 4505., 4517., 4532.,
            4431., 4526., 4534., 4528., 4513., 4476., 4272., 4526., 4496.,
            4536., 4535., 4537., 4532., 4524., 4535., 4464., 4490., 4535.,
            4438., 4525., 4538., 4515., 4429., 4533., 4251., 4502., 4491.,
            4520., 4465., 4526., 4537., 4533., 4534., 4522., 4533., 4525.,
            4526., 4535., 4528., 4528., 4532., 4525., 4500., 4538., 4534.,
            4533., 4473., 4493., 4402., 4531., 4537., 4533., 4496., 4505.,
            4491., 4379., 4511., 4537., 4536., 4508., 4428., 4376., 4538.,
            4536., 4538., 4509., 4485., 4515., 4510., 4537., 4535., 4537.,
            4523., 4527., 4454., 4511., 4520., 4538., 4496., 4520., 4533.,
            4518., 4533., 4538., 4526., 4533., 4516., 4524., 4523., 4533.,
            4537., 4536., 4534., 4536., 4536., 4537., 4533., 4528., 4530.,
            4503., 4530., 4525., 4508., 4527., 4491., 4522., 4423., 4395.,
            4533., 4523., 4536., 4530.]

    Accuracy (subset): 0.0
    Accuracy (ML): 0.9942644213062529
    Precision (macro): 1.0
    Precision (micro): 1.0
    Recall (macro): 0.9778358479072684
    Recall (micro): 0.0
    Hamming loss: 0.005734817590564489

DeBERTa:
    Correct predictions (499 classes):
        [4509., 4535., 4476., 4516., 4533., 4523., 4515., 4537., 4494.,
            4522., 4532., 4533., 4530., 4534., 4537., 4533., 4524., 4538.,
            4499., 4488., 4538., 4516., 4504., 4521., 4516., 4533., 4522.,
            4538., 4528., 4536., 4538., 4537., 4534., 4538., 4499., 4536.,
            4511., 4522., 4529., 4494., 4536., 4529., 4537., 4519., 4538.,
            4508., 4530., 4524., 4490., 4535., 4457., 4534., 4536., 4528.,
            4528., 4530., 4534., 4532., 4525., 4502., 4532., 4529., 4524.,
            4536., 4534., 4537., 4531., 4499., 4534., 4462., 4535., 4538.,
            4527., 4537., 4533., 4528., 4530., 4534., 4525., 4474., 4534.,
            4534., 4531., 4533., 4507., 4323., 4492., 4537., 4470., 4535.,
            4531., 4529., 4518., 4529., 4529., 4479., 4537., 4538., 4503.,
            4535., 4530., 4537., 4530., 4448., 4524., 4529., 4534., 4538.,
            4534., 4397., 4533., 4525., 4538., 4445., 4525., 4313., 4536.,
            4482., 4536., 4510., 4538., 4516., 4354., 4521., 4538., 4518.,
            4532., 4538., 4522., 4523., 4534., 4537., 4537., 4500., 4529.,
            4532., 4523., 4487., 4513., 4538., 4533., 4534., 4538., 4317.,
            4525., 4537., 4537., 4525., 4532., 4270., 4519., 4533., 4524.,
            4477., 4523., 4537., 4527., 4527., 4520., 4501., 4530., 4525.,
            4537., 4536., 4456., 4509., 4533., 4497., 4528., 4537., 4533.,
            4522., 4487., 4459., 4518., 4499., 4502., 4516., 4537., 4537.,
            4506., 4509., 4536., 4531., 4527., 4536., 4532., 4380., 4370.,
            4533., 4520., 4520., 4537., 4417., 4516., 4534., 4494., 4534.,
            4445., 4536., 4517., 4516., 4458., 4535., 4534., 4534., 4527.,
            4537., 4501., 4493., 4533., 4534., 4520., 4524., 4537., 4496.,
            4482., 4518., 4524., 4518., 4533., 4483., 4501., 4531., 4525.,
            4536., 4529., 4492., 4538., 4459., 4534., 4474., 4538., 4508.,
            4537., 4460., 4524., 4509., 4525., 4285., 4509., 4491., 4532.,
            4528., 4538., 4526., 4531., 4537., 4514., 4490., 4535., 4505.,
            4538., 4502., 4529., 4528., 4411., 4495., 4519., 4532., 4516.,
            4523., 4537., 4528., 4521., 4537., 4476., 4538., 4515., 4537.,
            4537., 4531., 4529., 4535., 4535., 4529., 4531., 4538., 4457.,
            4532., 4518., 4507., 4536., 4535., 4537., 4509., 4525., 4536.,
            4511., 4537., 4533., 4538., 4536., 4520., 4332., 4538., 4530.,
            4471., 4531., 4538., 4538., 4532., 4520., 4482., 4510., 4528.,
            4407., 4530., 4507., 4538., 4532., 4535., 4535., 4509., 4528.,
            4515., 4492., 4356., 4535., 4536., 4500., 4512., 4538., 4524.,
            4265., 4537., 4536., 4537., 4528., 4519., 4537., 4531., 4512.,
            4536., 4534., 4522., 4535., 4525., 4536., 4534., 4531., 4538.,
            4518., 4508., 4511., 4532., 4515., 4518., 4532., 4536., 4535.,
            4483., 4380., 4534., 4525., 4528., 4532., 4509., 4528., 4523.,
            4515., 4511., 4537., 4528., 4526., 4534., 4529., 4434., 4527.,
            4525., 4503., 4529., 4536., 4520., 4503., 4536., 4536., 4535.,
            4518., 4532., 4534., 4537., 4528., 4527., 4505., 4517., 4532.,
            4431., 4526., 4534., 4528., 4513., 4476., 4376., 4526., 4507.,
            4536., 4535., 4537., 4532., 4524., 4535., 4465., 4490., 4535.,
            4436., 4525., 4538., 4515., 4443., 4533., 4251., 4502., 4491.,
            4520., 4465., 4526., 4537., 4533., 4534., 4522., 4533., 4525.,
            4526., 4535., 4528., 4528., 4532., 4525., 4500., 4538., 4534.,
            4533., 4473., 4493., 4402., 4531., 4537., 4533., 4496., 4505.,
            4491., 4399., 4511., 4537., 4536., 4508., 4428., 4386., 4538.,
            4536., 4538., 4509., 4485., 4515., 4510., 4537., 4535., 4537.,
            4523., 4527., 4454., 4511., 4520., 4538., 4496., 4520., 4533.,
            4518., 4533., 4538., 4526., 4533., 4516., 4524., 4523., 4533.,
            4537., 4536., 4534., 4536., 4536., 4537., 4533., 4528., 4530.,
            4503., 4530., 4525., 4508., 4527., 4491., 4522., 4423., 4395.,
            4533., 4523., 4536., 4530.]

    Accuracy (subset): 0.0035257823765277863
    Accuracy (ML): 0.9945563228705098
    Precision (macro): 0.9998343977509889
    Precision (micro): 0.9328632290289407
    Recall (macro): 0.9947219251195205
    Recall (micro): 0.08561459841759533
    Hamming loss: 0.005443676840513945